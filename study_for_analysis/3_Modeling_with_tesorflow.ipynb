{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_gtk3agg.py:18: UserWarning: The Gtk3Agg backend is known to not work on Python 3.x with pycairo. Try installing cairocffi.\n",
      "  \"The Gtk3Agg backend is known to not work on Python 3.x with pycairo. \"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Image\n",
    "from pandas import get_dummies\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Config the matlotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/forward.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data0 = data[data['rating'] >= 6.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "data0 = data0.drop(['flag', 'full_time', 'half_time', 'league', 'mins', 'motm', 'name','player_number', 'position', 'ps_y', 'rating', 'team_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data0 = data0.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(data0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize data with Seaborn\n",
    "# g = sns.pairplot(data0, hue=\"labeled_league\", size=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = data0.columns\n",
    "features = cols[0:-1]\n",
    "labels = cols[27]\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Well conditioned data will have zero mean and equal variance\n",
    "# We get this automattically when we calculate the Z scores for the data\n",
    "\n",
    "data_norm = pd.DataFrame(data0)\n",
    "\n",
    "for feature in features:\n",
    "    data0[feature] = (data0[feature] - data0[feature].mean())/data0[feature].std()\n",
    "    \n",
    "# show that should now have zero mean\n",
    "print(\"Averages\")\n",
    "print(data0.mean())\n",
    "\n",
    "print(\"\\n Deviations\")\n",
    "\n",
    "print(pow(data0.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "indices = data_norm.index.tolist()\n",
    "indices = np.array(indices)\n",
    "np.random.shuffle(indices)\n",
    "X = data_norm.reindex(indices)[features]\n",
    "y = data_norm.reindex(indices)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One hot encode as a dataframe\n",
    "y = get_dummies(y)\n",
    "\n",
    "# Generate Traning and Validation Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Convert to np arrays so that we can use with Tensorflow\n",
    "X_train = np.array(X_train).astype(np.float32)\n",
    "X_test  = np.array(X_test).astype(np.float32)\n",
    "y_train = np.array(y_train).astype(np.float32)\n",
    "y_test  = np.array(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(X_train), np.shape(y_train))\n",
    "print(np.shape(X_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_size = X_train.shape[1]\n",
    "test_size = X_test.shape[1]\n",
    "num_features = 27\n",
    "num_labels = 6\n",
    "\n",
    "\n",
    "num_hidden = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_set    = tf.constant(X_train)\n",
    "    tf_train_labels = tf.constant(y_train)\n",
    "    tf_valid_set    = tf.constant(X_test)\n",
    " \n",
    "    \n",
    "    print(tf_train_set)\n",
    "    print(tf_train_labels)\n",
    "    \n",
    "    ## Note, since there is only 1 layer there are actually no hidden layers... but if there were\n",
    "    ## there would be num_hidden\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([num_features, num_hidden]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    ## tf.zeros Automaticaly adjusts rows to input data batch size\n",
    "    bias_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "    bias_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_set , weights_1 ) + bias_1\n",
    "    rel_1 = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(rel_1, weights_2) + bias_2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_2, labels=tf_train_labels))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.005).minimize(loss)\n",
    "    \n",
    "    \n",
    "    ## Training prediction\n",
    "    predict_train = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation prediction\n",
    "    logits_1_val = tf.matmul(tf_valid_set, weights_1) + bias_1\n",
    "    rel_1_val    = tf.nn.relu(logits_1_val)\n",
    "    logits_2_val = tf.matmul(rel_1_val, weights_2) + bias_2\n",
    "    predict_valid = tf.nn.softmax(logits_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 150000\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print(loss.eval())\n",
    "    for step in range(num_steps):\n",
    "        _,l, predictions = session.run([optimizer, loss, predict_train])\n",
    "        \n",
    "        if (step % 10000 == 0):\n",
    "#               print(predictions[3:6])\n",
    "              print('Loss at step %d: %f' % (step, l))\n",
    "              print('Training accuracy: %.1f%%' % accuracy( predictions, y_train[:, :]))\n",
    "#               print('Validation accuracy: %.1f%%' % accuracy(predict_valid.eval(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 포지션 다돌려보자ㅋㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "forward_df = pd.read_csv(\"./data/forward.csv\", index_col=0)\n",
    "mid_df = pd.read_csv(\"./data/mid.csv\", index_col=0)\n",
    "defense_df = pd.read_csv(\"./data/defense.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step0 : 평점으로 안나눳더니 별로 안좋음\n",
    "# step1 : 평점 6.7이상만 돌려봅시다\n",
    "# step2 : validation 의 정확도 체크는 실제로 높은 색기들이 어디에 속하는지 진짜로 알아보는거잔아 그치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.concat([forward_df, mid_df, defense_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2935.000000\n",
       "mean        6.680763\n",
       "std         0.382800\n",
       "min         5.310000\n",
       "25%         6.410000\n",
       "50%         6.700000\n",
       "75%         6.930000\n",
       "max         8.460000\n",
       "Name: rating, dtype: float64"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data0 = data[data['rating'] >= 6.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data0 = data0.drop(['flag', 'full_time', 'half_time', 'league', 'mins', 'motm', 'name','player_number', 'position', 'ps_y', 'rating', 'team_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fillna to zero\n",
    "data0 = data0.fillna(0.5)\n",
    "valid_data0 = valid_data0.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'asists', 'avgp', 'aw', 'blocks', 'clear', 'crosses', 'disp',\n",
       "       'drb', 'fouled', 'fouls', 'goals', 'inter', 'keyp', 'longb', 'off',\n",
       "       'offsides', 'owng', 'ps_x', 'red', 'spg', 'tackles', 'tall', 'thrb',\n",
       "       'unstch', 'weight', 'yel', 'labeled_league'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'asists', 'avgp', 'aw', 'blocks', 'clear', 'crosses', 'disp',\n",
      "       'drb', 'fouled', 'fouls', 'goals', 'inter', 'keyp', 'longb', 'off',\n",
      "       'offsides', 'owng', 'ps_x', 'red', 'spg', 'tackles', 'tall', 'thrb',\n",
      "       'unstch', 'weight', 'yel'],\n",
      "      dtype='object') labeled_league\n"
     ]
    }
   ],
   "source": [
    "cols = data0.columns\n",
    "features = cols[0:-1]\n",
    "labels = cols[27]\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages\n",
      "age              -1.438646e-16\n",
      "asists            2.934827e-15\n",
      "avgp              6.261177e-16\n",
      "aw                8.227151e-17\n",
      "blocks           -1.256410e-15\n",
      "clear             1.449842e-16\n",
      "crosses           1.220381e-15\n",
      "disp             -3.271509e-16\n",
      "drb               1.398837e-17\n",
      "fouled           -4.587963e-16\n",
      "fouls             7.408859e-18\n",
      "goals            -2.383939e-15\n",
      "inter            -2.999537e-15\n",
      "keyp              6.261177e-16\n",
      "longb             2.507401e-17\n",
      "off               1.428196e-15\n",
      "offsides          6.426909e-16\n",
      "owng              1.502063e-15\n",
      "ps_x             -8.549768e-16\n",
      "red              -2.087197e-16\n",
      "spg              -2.725797e-17\n",
      "tackles           1.217486e-16\n",
      "tall              3.835052e-16\n",
      "thrb             -8.254796e-17\n",
      "unstch           -1.071796e-15\n",
      "weight            5.698186e-16\n",
      "yel              -5.819272e-16\n",
      "labeled_league    2.456175e+00\n",
      "dtype: float64\n",
      "\n",
      " Deviations\n",
      "age               1.000000\n",
      "asists            1.000000\n",
      "avgp              1.000000\n",
      "aw                1.000000\n",
      "blocks            1.000000\n",
      "clear             1.000000\n",
      "crosses           1.000000\n",
      "disp              1.000000\n",
      "drb               1.000000\n",
      "fouled            1.000000\n",
      "fouls             1.000000\n",
      "goals             1.000000\n",
      "inter             1.000000\n",
      "keyp              1.000000\n",
      "longb             1.000000\n",
      "off               1.000000\n",
      "offsides          1.000000\n",
      "owng              1.000000\n",
      "ps_x              1.000000\n",
      "red               1.000000\n",
      "spg               1.000000\n",
      "tackles           1.000000\n",
      "tall              1.000000\n",
      "thrb              1.000000\n",
      "unstch            1.000000\n",
      "weight            1.000000\n",
      "yel               1.000000\n",
      "labeled_league    2.736494\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_norm = pd.DataFrame(data0)\n",
    "\n",
    "for feature in features:\n",
    "    data0[feature] = (data0[feature] - data0[feature].mean())/data0[feature].std()\n",
    "    \n",
    "# show that should now have zero mean\n",
    "print(\"Averages\")\n",
    "print(data0.mean())\n",
    "\n",
    "print(\"\\n Deviations\")\n",
    "\n",
    "print(pow(data0.std(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle the data\n",
    "indices = data_norm.index.tolist()\n",
    "indices = np.array(indices)\n",
    "np.random.shuffle(indices)\n",
    "X = data_norm.reindex(indices)[features]\n",
    "y = data_norm.reindex(indices)[labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One hot encode as a dataframe\n",
    "y = get_dummies(y)\n",
    "\n",
    "# Generate Traning and Validation Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Convert to np arrays so that we can use with Tensorflow\n",
    "X_train = np.array(X_train).astype(np.float32)\n",
    "X_test  = np.array(X_test).astype(np.float32)\n",
    "y_train = np.array(y_train).astype(np.float32) \n",
    "y_test  = np.array(y_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1606, 27) (1606, 6)\n",
      "(402, 27) (402, 6)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train), np.shape(y_train))\n",
    "print(np.shape(X_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1606, 27), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(1606, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "training_size = X_train.shape[1]\n",
    "test_size = X_test.shape[1]\n",
    "num_features = 27\n",
    "num_labels = 6\n",
    "\n",
    "\n",
    "num_hidden = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_set    = tf.constant(X_train)\n",
    "    tf_train_labels = tf.constant(y_train)\n",
    "    tf_valid_set    = tf.constant(X_test)\n",
    " \n",
    "    \n",
    "    print(tf_train_set)\n",
    "    print(tf_train_labels)\n",
    "    \n",
    "    ## Note, since there is only 1 layer there are actually no hidden layers... but if there were\n",
    "    ## there would be num_hidden\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([num_features, num_hidden]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    ## tf.zeros Automaticaly adjusts rows to input data batch size\n",
    "    bias_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "    bias_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_set , weights_1 ) + bias_1\n",
    "    rel_1 = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(rel_1, weights_2) + bias_2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_2, labels=tf_train_labels))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(.1).minimize(loss)\n",
    "    \n",
    "    \n",
    "    ## Training prediction\n",
    "    predict_train = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation prediction\n",
    "    logits_1_val = tf.matmul(tf_valid_set, weights_1) + bias_1\n",
    "    rel_1_val    = tf.nn.relu(logits_1_val)\n",
    "    logits_2_val = tf.matmul(rel_1_val, weights_2) + bias_2\n",
    "    predict_valid = tf.nn.softmax(logits_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2094\n",
      "Loss at step 0: 11.209355\n",
      "Training accuracy: 15.8%\n",
      "Validation accuracy: 20.9%\n",
      "Loss at step 1000: 1.545758\n",
      "Training accuracy: 37.3%\n",
      "Validation accuracy: 32.1%\n",
      "Loss at step 2000: 1.454501\n",
      "Training accuracy: 41.6%\n",
      "Validation accuracy: 35.1%\n",
      "Loss at step 3000: 1.406107\n",
      "Training accuracy: 44.4%\n",
      "Validation accuracy: 36.8%\n",
      "Loss at step 4000: 1.375209\n",
      "Training accuracy: 45.2%\n",
      "Validation accuracy: 38.1%\n",
      "Loss at step 5000: 1.351617\n",
      "Training accuracy: 47.6%\n",
      "Validation accuracy: 38.1%\n",
      "Loss at step 6000: 1.334269\n",
      "Training accuracy: 48.6%\n",
      "Validation accuracy: 39.1%\n",
      "Loss at step 7000: 1.319582\n",
      "Training accuracy: 50.2%\n",
      "Validation accuracy: 38.8%\n",
      "Loss at step 8000: 1.307727\n",
      "Training accuracy: 50.4%\n",
      "Validation accuracy: 39.6%\n",
      "Loss at step 9000: 1.297484\n",
      "Training accuracy: 51.3%\n",
      "Validation accuracy: 40.3%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-291-10481088f58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "with tf.Session(graph = graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(loss.eval())\n",
    "    for step in range(num_steps):\n",
    "        _,l, predictions = session.run([optimizer, loss, predict_train])\n",
    "        \n",
    "        if (step % 1000 == 0 or step == num_steps -1):\n",
    "              print('Loss at step %d: %f' % (step, l))\n",
    "              print('Training accuracy: %.1f%%' % accuracy( predictions, y_train[:, :]))\n",
    "              print('Validation accuracy: %.1f%%' % accuracy(predict_valid.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1606, 27), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(1606, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "training_size = X_train.shape[1]\n",
    "test_size = X_test.shape[1]\n",
    "num_features = 27\n",
    "num_labels = 6\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "\n",
    "num_hidden = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_set    = tf.constant(X_train)\n",
    "    tf_train_labels = tf.constant(y_train)\n",
    "    tf_valid_set    = tf.constant(X_test)\n",
    " \n",
    "    \n",
    "    print(tf_train_set)\n",
    "    print(tf_train_labels)\n",
    "    \n",
    "    ## Note, since there is only 1 layer there are actually no hidden layers... but if there were\n",
    "    ## there would be num_hidden\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([num_features, num_hidden]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    ## tf.zeros Automaticaly adjusts rows to input data batch size\n",
    "    bias_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "    bias_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_set , weights_1 ) + bias_1\n",
    "    rel_1 = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(rel_1, weights_2) + bias_2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_2, labels=tf_train_labels))\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    ## Training prediction\n",
    "    predict_train = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation prediction\n",
    "    logits_1_val = tf.matmul(tf_valid_set, weights_1) + bias_1\n",
    "    rel_1_val    = tf.nn.relu(logits_1_val)\n",
    "    logits_2_val = tf.matmul(rel_1_val, weights_2) + bias_2\n",
    "    predict_valid = tf.nn.softmax(logits_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3682.87\n",
      "Loss at step 0: 3682.865479\n",
      "Training accuracy: 18.3%\n",
      "Tensor(\"Softmax_1:0\", shape=(402, 6), dtype=float32)\n",
      "Validation accuracy: 15.2%\n",
      "Loss at step 1000: 1.786591\n",
      "Training accuracy: 18.7%\n",
      "Tensor(\"Softmax_1:0\", shape=(402, 6), dtype=float32)\n",
      "Validation accuracy: 19.4%\n",
      "Loss at step 2000: 1.786590\n",
      "Training accuracy: 18.7%\n",
      "Tensor(\"Softmax_1:0\", shape=(402, 6), dtype=float32)\n",
      "Validation accuracy: 19.4%\n",
      "Loss at step 3000: 1.786591\n",
      "Training accuracy: 18.7%\n",
      "Tensor(\"Softmax_1:0\", shape=(402, 6), dtype=float32)\n",
      "Validation accuracy: 19.4%\n",
      "Loss at step 4000: 1.786591\n",
      "Training accuracy: 18.7%\n",
      "Tensor(\"Softmax_1:0\", shape=(402, 6), dtype=float32)\n",
      "Validation accuracy: 19.4%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-274-1fc63fad0cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "with tf.Session(graph = graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(loss.eval())\n",
    "    for step in range(num_steps):\n",
    "        _,l, predictions = session.run([optimizer, loss, predict_train])\n",
    "        \n",
    "        if (step % 1000 == 0 or step == num_steps -1):\n",
    "          print('Loss at step %d: %f' % (step, l))\n",
    "          print('Training accuracy: %.1f%%' % accuracy( predictions, y_train[:, :]))\n",
    "          print(predict_valid)\n",
    "          print('Validation accuracy: %.1f%%' % accuracy(predict_valid.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이청용이랑 박주호의 리그를 추천받아보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data0['name'] = data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LCY = data[data['name'] == \"Lee Chung-yong\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PJH = data[data['name'] == \"Joo-Ho Park\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'asists', 'avgp', 'aw', 'blocks', 'clear', 'crosses', 'disp',\n",
       "       'drb', 'flag', 'fouled', 'fouls', 'full_time', 'goals', 'half_time',\n",
       "       'inter', 'keyp', 'league', 'longb', 'mins', 'motm', 'name', 'off',\n",
       "       'offsides', 'owng', 'player_number', 'position', 'ps_x', 'ps_y',\n",
       "       'rating', 'red', 'spg', 'tackles', 'tall', 'team_name', 'thrb',\n",
       "       'unstch', 'weight', 'yel', 'labeled_league'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PJH.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_col_list = ['flag', 'full_time', 'half_time', 'league', 'mins', 'motm', 'name', 'player_number', 'position', \n",
    "                 'ps_y', 'rating', 'team_name', 'labeled_league']\n",
    "\n",
    "#drop columns\n",
    "LCY0 = LCY.drop(drop_col_list, axis=1)\n",
    "PJH0 = PJH.drop(drop_col_list, axis=1)\n",
    "\n",
    "#get np arrays value\n",
    "LCY1 = np.array(LCY0).astype(np.float32)\n",
    "PJH1 = np.array(PJH0.mean()).astype(np.float32)\n",
    "\n",
    "#set target player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_players = np.vstack([LCY1, PJH1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(1606, 27), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(1606, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "training_size = X_train.shape[1]\n",
    "test_size = X_test.shape[1]\n",
    "num_features = 27\n",
    "num_labels = 6\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "\n",
    "num_hidden = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_set    = tf.constant(X_train)\n",
    "    tf_train_labels = tf.constant(y_train)\n",
    "    tf_valid_set    = tf.constant(target_players)\n",
    " \n",
    "    \n",
    "    print(tf_train_set)\n",
    "    print(tf_train_labels)\n",
    "    \n",
    "    ## Note, since there is only 1 layer there are actually no hidden layers... but if there were\n",
    "    ## there would be num_hidden\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([num_features, num_hidden]))\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n",
    "    ## tf.zeros Automaticaly adjusts rows to input data batch size\n",
    "    bias_1 = tf.Variable(tf.zeros([num_hidden]))\n",
    "    bias_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    logits_1 = tf.matmul(tf_train_set , weights_1 ) + bias_1\n",
    "    rel_1 = tf.nn.relu(logits_1)\n",
    "    logits_2 = tf.matmul(rel_1, weights_2) + bias_2\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_2, labels=tf_train_labels))\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "    \n",
    "    ## Training prediction\n",
    "    predict_train = tf.nn.softmax(logits_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Validation prediction\n",
    "    logits_1_val = tf.matmul(tf_valid_set, weights_1) + bias_1\n",
    "    rel_1_val    = tf.nn.relu(logits_1_val)\n",
    "    logits_2_val = tf.matmul(rel_1_val, weights_2) + bias_2\n",
    "    predict_valid = tf.nn.softmax(logits_2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_1:0' shape=(2, 6) dtype=float32>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.2872\n",
      "[[ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]]\n",
      "\n",
      " [[0 1 2 4 5 3]\n",
      " [0 1 2 4 5 3]]\n",
      "Loss at step 0: 10.287201\n",
      "Training accuracy: 17.6%\n",
      "[[  0.00000000e+00   0.00000000e+00   2.83884191e-37   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   3.44064960e-38   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 1000: 1.204025\n",
      "Training accuracy: 52.6%\n",
      "[[  0.00000000e+00   0.00000000e+00   2.89270850e-18   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.06987004e-19   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 2000: 1.197226\n",
      "Training accuracy: 53.1%\n",
      "[[  0.00000000e+00   0.00000000e+00   1.71901783e-20   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   2.93740449e-22   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 3000: 1.198940\n",
      "Training accuracy: 53.4%\n",
      "[[  0.00000000e+00   0.00000000e+00   2.14755012e-23   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   3.67886171e-25   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 4000: 1.197637\n",
      "Training accuracy: 53.6%\n",
      "[[  0.00000000e+00   0.00000000e+00   5.26469107e-22   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   8.43460177e-24   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 5000: 1.196680\n",
      "Training accuracy: 54.0%\n",
      "[[  0.00000000e+00   0.00000000e+00   4.33085241e-20   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   6.58770194e-22   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 6000: 1.196521\n",
      "Training accuracy: 53.1%\n",
      "[[  0.00000000e+00   0.00000000e+00   5.39030850e-19   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   8.25498867e-21   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 7000: 1.197409\n",
      "Training accuracy: 53.8%\n",
      "[[  0.00000000e+00   0.00000000e+00   9.84568019e-23   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.60124663e-24   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 8000: 1.197019\n",
      "Training accuracy: 53.5%\n",
      "[[  0.00000000e+00   0.00000000e+00   1.34639332e-20   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   2.18555996e-22   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 9000: 1.196397\n",
      "Training accuracy: 53.6%\n",
      "[[  0.00000000e+00   0.00000000e+00   2.06350119e-21   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   2.94216908e-23   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n",
      "\n",
      " [[0 1 4 5 2 3]\n",
      " [0 1 4 5 2 3]]\n",
      "Loss at step 9999: 1.197018\n",
      "Training accuracy: 53.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10000\n",
    "with tf.Session(graph = graph) as session:\n",
    "#     tf.initialize_all_variables().run()\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(loss.eval())\n",
    "    for step in range(num_steps):\n",
    "        _,l, predictions = session.run([optimizer, loss, predict_train])\n",
    "    \n",
    "        if (step % 1000 == 0 or step == num_steps -1):\n",
    "            print(predict_valid.eval())\n",
    "            print(\"\\n\", predict_valid.eval().argsort())\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy( predictions, y_train[:, :]))\n",
    "#             print('Validation accuracy: %.1f%%' % accuracy(predict_valid.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
